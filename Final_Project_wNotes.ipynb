{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Code","metadata":{"execution":{"iopub.status.busy":"2021-12-02T01:46:26.634268Z","iopub.execute_input":"2021-12-02T01:46:26.634845Z","iopub.status.idle":"2021-12-02T01:46:26.638445Z","shell.execute_reply.started":"2021-12-02T01:46:26.634806Z","shell.execute_reply":"2021-12-02T01:46:26.637791Z"}}},{"cell_type":"markdown","source":"**Standards:** \n\nWe use Python version 3.7.10 and scikit-learn packages for model training and prediction. Specifically, we use BernoulliNB, MultinomialNB, GaussianNB, svm, and KNeighborsClassifier packages from scikit-learn for our baseline models, and the MLPClassifier package for our experimental CNN model. Our accuracy measurements come from the scikit-learn accuracy_score and classification_report packages, and we find optimal parameters for our models using GridSearchCV.\n\nWe also utilize the pandas and numpy libraries, as well as train_test_split and StandardScaler packages from the scikit-learn library, for data preprocessing and management.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nimport time\n!python --version","metadata":{"execution":{"iopub.status.busy":"2021-12-04T15:18:33.568687Z","iopub.execute_input":"2021-12-04T15:18:33.569528Z","iopub.status.idle":"2021-12-04T15:18:35.840382Z","shell.execute_reply.started":"2021-12-04T15:18:33.569478Z","shell.execute_reply":"2021-12-04T15:18:35.839049Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Data Import \n\nImport data and take 10,000 samples from the data for model training and assessment, for time efficiency. The entire dataset has 35,887 rows; some of our models took over 9 hours to train on that much data. For this reason, we narrow the data by taking a sample in order to manage the amount of time spent on training.","metadata":{}},{"cell_type":"code","source":"# import data\n# all_data = pd.read_csv('icml_face_data.csv')\nall_data = pd.read_csv('../input/facial-expression-recognition-challenge/icml_face_data.csv/icml_face_data.csv')\n# pd.read_csv('/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv')\n# all_data = all_data[0:700] # just for dev... remove for actual training\nprint(all_data.shape)\nall_data = all_data.sample(n=10000, random_state=1)\n\naccuracy = {}\nparams = {}","metadata":{"execution":{"iopub.status.busy":"2021-12-04T15:29:02.440322Z","iopub.execute_input":"2021-12-04T15:29:02.440649Z","iopub.status.idle":"2021-12-04T15:29:05.421649Z","shell.execute_reply.started":"2021-12-04T15:29:02.440616Z","shell.execute_reply":"2021-12-04T15:29:05.420944Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\n\nAfter renaming the Usage and pixels columns for formatting, we define the function `pixels_to_arr` that convert a pandas Series of pixels to a numpy array of pixels, and apply it to the pixels column of our dataframe. This produces a one-dimensional array of 2,304 pixel values for each row of data.\n\nTo make those arrays more usable, we then define the function `image_reshape` that reshapes each pixel array to a 48x48 matrix. Finally, we define the X and Y values we will feed into our models using the reshaped pixel matrices and their corresponding labels.","metadata":{"execution":{"iopub.status.busy":"2021-12-04T15:31:01.338593Z","iopub.execute_input":"2021-12-04T15:31:01.338896Z","iopub.status.idle":"2021-12-04T15:31:01.344031Z","shell.execute_reply.started":"2021-12-04T15:31:01.338866Z","shell.execute_reply":"2021-12-04T15:31:01.342877Z"}}},{"cell_type":"code","source":"all_data.rename({' Usage': 'Usage', ' pixels': 'pixels'}, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T15:44:12.976967Z","iopub.execute_input":"2021-12-04T15:44:12.977616Z","iopub.status.idle":"2021-12-04T15:44:12.983460Z","shell.execute_reply.started":"2021-12-04T15:44:12.977579Z","shell.execute_reply":"2021-12-04T15:44:12.982548Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def pixels_to_arr(pixels):\n    array = np.array(pixels.split(),'float64')\n    return array\n\nall_data['pixels_arr'] = all_data['pixels'].apply(pixels_to_arr)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T15:44:12.984936Z","iopub.execute_input":"2021-12-04T15:44:12.985163Z","iopub.status.idle":"2021-12-04T15:44:21.511224Z","shell.execute_reply.started":"2021-12-04T15:44:12.985136Z","shell.execute_reply":"2021-12-04T15:44:21.510193Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def image_reshape(data):\n    image = np.reshape(data['pixels_arr'].to_list(),(data.shape[0],48,48,1))\n    return image\n\nX = image_reshape(all_data)\ny = all_data['emotion']","metadata":{"execution":{"iopub.status.busy":"2021-12-04T15:44:21.516341Z","iopub.execute_input":"2021-12-04T15:44:21.516654Z","iopub.status.idle":"2021-12-04T15:44:21.631325Z","shell.execute_reply.started":"2021-12-04T15:44:21.516619Z","shell.execute_reply":"2021-12-04T15:44:21.630313Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"#### Prepare train and test sets\n\nHere we create train and test sets using our 10,000-row dataframe. The training set has 8,000 rows and the testing set has 2,000 rows, and each contains 2,304 pixel values per row, unraveled so that each value is in its own column of the dataframe. This format is used because it is better than a single array of pixel values for training models.","metadata":{}},{"cell_type":"code","source":"x_unraveled = pd.DataFrame(list(map(np.ravel, all_data['pixels_arr'])))\nX_train_unrav, X_test_unrav, y_train_unrav, y_test_unrav = train_test_split(x_unraveled, y, test_size=0.2, random_state=12345)\nprint(\"Pixels as columns\")\nprint(\"Training data shape: \", X_train_unrav.shape)\nprint(\"Test data shape\", X_test_unrav.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T15:58:14.931680Z","iopub.execute_input":"2021-12-04T15:58:14.932267Z","iopub.status.idle":"2021-12-04T15:58:39.927698Z","shell.execute_reply.started":"2021-12-04T15:58:14.932214Z","shell.execute_reply":"2021-12-04T15:58:39.926437Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Data Examination\n\nThis is a preliminary examination of what is contained in our data. The first code block produces a bar chart showing the proportions of each labeled emotion within the 10,000-row dataframe. There is a notably higher proportion of \"happy\" samples than there are any other emotion, and a very low proportion of the samples are labeled \"disgust\". The rest of the labels comprise between 10-20% of the limited dataframe.\n\nThe next block displays 5 samples with each label.","metadata":{}},{"cell_type":"code","source":"emotion_prop = (all_data.emotion.value_counts() / len(all_data)).to_frame().sort_index(ascending=True)\n\nemotions = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\npalette = ['orchid', 'lightcoral', 'orange', 'gold', 'lightgreen', 'deepskyblue', 'cornflowerblue']\n\nplt.figure(figsize=[12,6])\n\nplt.bar(x=emotions, height=emotion_prop['emotion'], color=palette, edgecolor='black')\n    \nplt.xlabel('Emotion')\nplt.ylabel('Proportion')\nplt.title('Emotion Label Proportions')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-04T15:39:30.261643Z","iopub.execute_input":"2021-12-04T15:39:30.261988Z","iopub.status.idle":"2021-12-04T15:39:30.549468Z","shell.execute_reply.started":"2021-12-04T15:39:30.261958Z","shell.execute_reply":"2021-12-04T15:39:30.548467Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"row = 0\nfor emotion in list(range(7)):\n\n    all_emotion_images = all_data[all_data['emotion'] == emotion]\n    for i in range(5):\n        \n        img = all_emotion_images.iloc[i,].pixels_arr.reshape(48,48)\n        lab = emotions[emotion]\n        \n        plt.subplot(7,5,row+i+1)\n        plt.imshow(img, cmap='binary_r')\n        plt.axis('off')\n    plt.text(-600, 27, s = str(lab), fontsize=10)\n    row += 5\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-04T15:47:25.138667Z","iopub.execute_input":"2021-12-04T15:47:25.139021Z","iopub.status.idle":"2021-12-04T15:47:26.812176Z","shell.execute_reply.started":"2021-12-04T15:47:25.138987Z","shell.execute_reply":"2021-12-04T15:47:26.811497Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Models\n\nWe train and measure the prediction accuracy of 5 baseline models to later compare to our experimental model. Each model is trained on the unraveled training data, and tested on the unraveled testing data. We use GridSearchCV to perform a 5-fold cross-validated grid search for the optimal parameters with regard to accuracy for each model. The `accuracy` scoring parameter passed into the GridSearchCV function calls the `accuracy_score` function of scikit-learn, which returns the fraction of correctly classified samples in the test set. This is the value we use for each model's accuracy as a whole.","metadata":{}},{"cell_type":"markdown","source":"## K Nearest Neighbors Model\n\nThe grid search for KNN model parameters includes all values of n between 1 and 19. It finds that the optimal number of neighbors for the model is 18; a model trained with these parameters is 26.43% accurate.","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(algorithm='auto')\nparam_grid = dict(n_neighbors=list(range(1, 20)))\nmodel_KNN = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy',verbose=1)\n\nmodel_KNN.fit(X_train_unrav, y_train)\ny_pred_KNN = model_KNN.predict(X_test_unrav)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model_KNN.best_params_)\n# n_neighbors = 18\n# The model is 26.43% accurate","metadata":{"execution":{"iopub.status.busy":"2021-12-03T20:11:41.895827Z","iopub.execute_input":"2021-12-03T20:11:41.896173Z","iopub.status.idle":"2021-12-03T20:11:41.920835Z","shell.execute_reply.started":"2021-12-03T20:11:41.896137Z","shell.execute_reply":"2021-12-03T20:11:41.919529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy['KNN'] = accuracy_score(y_pred_KNN, y_test)\nparams['KNN'] = model_KNN.best_params_\nprint(f\"The model is {accuracy['KNN']*100:.2f}% accurate\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T20:11:21.261557Z","iopub.status.idle":"2021-12-03T20:11:21.262136Z","shell.execute_reply.started":"2021-12-03T20:11:21.261819Z","shell.execute_reply":"2021-12-03T20:11:21.261849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM Model","metadata":{}},{"cell_type":"markdown","source":"The grid search for SVM model parameters includes 5 possible values for the regularization parameter ('C'), four possible values for the kernel coefficient ('gamma'), and three possible kernel types. \n\nThe best possible parameters from these options are a regularization paremeter of 0.1, a gamma value of 0.0001 and a linear kernel type. An SVC model from the scikit-learn SVM package trained with these parameters is 25% accurate.","metadata":{}},{"cell_type":"code","source":"# param_grid = {'C':[1,10],'gamma':[0.001,0.1],'kernel':['rbf','poly']}\nparam_grid = {'C':[0.01,0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly', 'linear']}\nsvc = svm.SVC(probability=True)\nprint(\"The training of the model is started, please wait for while as it may take few minutes to complete\")\n\nmodel_SVM = GridSearchCV(svc,param_grid)\n\nstart = time.time()\nmodel_SVM.fit(X_train_unrav,y_train)\nend = time.time()\n\nprint(f\"Train time {end-start}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The Model is trained well with the given images')\nprint(model_SVM.best_params_)\n# {'C': 0.1, 'gamma': 0.0001, 'kernel': 'linear'}\n# The model is 25.00% accurate ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_SVM = model_SVM.predict(X_test_unrav)\naccuracy['SVM'] = accuracy_score(y_pred_SVM, y_test)\nparams['SVM'] = model_SVM.best_params_\n\nprint(f\"The model is {accuracy['SVM']*100:.2f}% accurate\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bernoulli Naive Bayes Model\n\nA grid search for Bernoulli Naive Bayes parameters searches for the best smoothing parameter ('alpha') from a list of 9 possibilities between 0 and 10. The model is given a binarization value of 0.1, meaning that any pixel values less than 0.1 are replaced by 0, and any above 0.1 are replaced by 1. (***QUESTION***: how did we decide on this value?) \n\nIn this configuration, the optimal alpha value is 0.5. A model with an alpha parameter of 0.5 is 25.55% accurate.","metadata":{}},{"cell_type":"code","source":"alphas = {'alpha': [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n\nbnb = BernoulliNB(binarize=0.1)\nmodel_BNB = GridSearchCV(bnb, alphas, scoring='accuracy')\nmodel_BNB.fit(X_train_unrav, y_train) \n\ny_pred_BNB = model_BNB.predict(X_test_unrav) \naccuracy['BNB'] = accuracy_score(y_pred_BNB,y_test)\nparams['BNB'] = model_BNB.best_params_\n\nprint(f\"The model is {accuracy['BNB']*100:.2f}% accurate\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T20:17:28.792904Z","iopub.execute_input":"2021-12-03T20:17:28.793535Z","iopub.status.idle":"2021-12-03T20:17:52.767903Z","shell.execute_reply.started":"2021-12-03T20:17:28.79348Z","shell.execute_reply":"2021-12-03T20:17:52.766968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model_BNB.best_params_)\n# {'alpha': 0.5}\n# The model is 25.55% accurate ","metadata":{"execution":{"iopub.status.busy":"2021-12-03T20:20:39.760929Z","iopub.execute_input":"2021-12-03T20:20:39.761279Z","iopub.status.idle":"2021-12-03T20:20:39.767059Z","shell.execute_reply.started":"2021-12-03T20:20:39.761241Z","shell.execute_reply":"2021-12-03T20:20:39.765889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multinomial Naive Bayes Model\n\nA grid search for Multinomial Naive Bayes parameters also searches for the best smoothing parameter ('alpha') from the same list of 9 possibilities between 0 and 10 that was used for the Bernoulli Naive Bayes grid search. Data fed into the model is trinarized with thresholds of 0.25 and 0.75. This means that any pixel values less than 0.25 are replaced by 0, any between 0.25 and 0.75 are replaced by 1, and any above 0.75 are replaced by 2. (***QUESTION***: how did we decide on these values?) \n\nIn this configuration, the optimal alpha value is 1e-10, and the resulting model is 25.30% accurate.","metadata":{}},{"cell_type":"code","source":"# GRID SEARCH\ndef trinarize(data, lower, upper):\n        trinarized_data = np.zeros(data.shape)\n        trinarized_data[(data <= lower)] = 0\n        trinarized_data[(data > lower)] = 1\n        trinarized_data[(data >= upper)] = 2\n        return trinarized_data\n\nalphas = {'alpha': [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n\nmnb = MultinomialNB()\nmodel_MNB = GridSearchCV(mnb, alphas, scoring='accuracy')\nmodel_MNB.fit(trinarize(X_train_unrav,0.25,0.75), y_train)\ny_pred_MNB = model_MNB.predict(trinarize(X_test_unrav,0.25,0.75))\naccuracy['MNB'] = accuracy_score(y_pred_MNB,y_test)\nparams['MNB'] = model_MNB.best_params_\n\nprint(f\"The Naive Bayes model is {accuracy['MNB']*100:.2f}% accurate\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T20:27:24.003489Z","iopub.execute_input":"2021-12-03T20:27:24.003803Z","iopub.status.idle":"2021-12-03T20:27:31.880881Z","shell.execute_reply.started":"2021-12-03T20:27:24.003771Z","shell.execute_reply":"2021-12-03T20:27:31.87951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model_MNB.best_params_)\n# {'alpha': 1e-10}\n# The Naive Bayes model is 25.30% accurate","metadata":{"execution":{"iopub.status.busy":"2021-12-03T20:27:31.995092Z","iopub.execute_input":"2021-12-03T20:27:31.996321Z","iopub.status.idle":"2021-12-03T20:27:32.003447Z","shell.execute_reply.started":"2021-12-03T20:27:31.996249Z","shell.execute_reply":"2021-12-03T20:27:32.002673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gaussian Naive Bayes Model\n\nA grid search for Gaussian Naive Bayes parameters searches for the best smoothing variance, a portion of the largest variance of all the features, to add to each feature variance. This value is chosen from a collection of 100 possibilities in the log space between 0 and -9. \n\nThe optimal smoothing variance is ~0.0023, and the resulting model is 21.75% accurate.","metadata":{}},{"cell_type":"code","source":"#GRID SEARCH\nalphas = {'alpha': [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\nparam_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\n\ngnb2 = GaussianNB()\nmodel_GNB2 = GridSearchCV(gnb2, param_grid, scoring='accuracy')\nmodel_GNB2.fit(X_train_unrav, y_train)\ny_pred_GNB2 = model_GNB2.predict(X_test_unrav)\naccuracy['GNB'] = accuracy_score(y_pred_GNB2,y_test)\nparams['GNB'] = model_GNB2.best_params_\n\nprint(f\"Gaussian Naive Bayes model is {accuracy['GNB']*100:.2f}% accurate\")","metadata":{"execution":{"iopub.status.busy":"2021-12-04T16:56:53.055540Z","iopub.execute_input":"2021-12-04T16:56:53.055819Z","iopub.status.idle":"2021-12-04T17:03:30.518441Z","shell.execute_reply.started":"2021-12-04T16:56:53.055790Z","shell.execute_reply":"2021-12-04T17:03:30.517273Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print(model_GNB2.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T17:03:30.520230Z","iopub.execute_input":"2021-12-04T17:03:30.520498Z","iopub.status.idle":"2021-12-04T17:03:30.526484Z","shell.execute_reply.started":"2021-12-04T17:03:30.520468Z","shell.execute_reply":"2021-12-04T17:03:30.525058Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## CNN Model ","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train_unrav)\nX_train_unrav = scaler.transform(X_train_unrav)\nX_test_unrav = scaler.transform(X_test_unrav)\nparam_grid = {'alpha': 10.0 ** -np.arange(1, 7),\n             'solver': ['lbfgs', 'sgd', 'adam'],\n             'hidden_layer_sizes': [250, 100, 50, 25, 5]}\nclf = GridSearchCV(MLPClassifier(random_state=12345), param_grid)\nstart = time.time()\nclf.fit(X_train_unrav, y_train_unrav)\nend = time.time()\nprint(f\"Train time: {start-end}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T20:28:09.120645Z","iopub.execute_input":"2021-12-03T20:28:09.12103Z","iopub.status.idle":"2021-12-03T20:28:18.428548Z","shell.execute_reply.started":"2021-12-03T20:28:09.120973Z","shell.execute_reply":"2021-12-03T20:28:18.426288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\nprint(\"Best parameters set found on training set:\")\nprint()\nprint(clf.best_params_)\nprint()\nprint(\"Grid scores on training set:\")\nprint()\nmeans = clf.cv_results_[\"mean_test_score\"]\nstds = clf.cv_results_[\"std_test_score\"]\nfor mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Detailed classification report for optimal parameters:\")\nprint()\nprint(\"The model is trained on the full training set.\")\nprint(\"The scores are computed on the full test set.\")\nprint()\ny_true, y_pred = y_test_unrav, clf.predict(X_test_unrav)\nprint(classification_report(y_true, y_pred))\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['CLF'] = clf.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_df = pd.DataFrame(list(accuracy.items()))\nparams_df = pd.DataFrame(list(params.items()))\n\naccuracy_df.to_csv('accuracy_df.csv', index=False)\nparams_df =.to_csv('params_df.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T03:12:54.584208Z","iopub.execute_input":"2021-12-01T03:12:54.584585Z","iopub.status.idle":"2021-12-01T03:12:54.592362Z","shell.execute_reply.started":"2021-12-01T03:12:54.584545Z","shell.execute_reply":"2021-12-01T03:12:54.591595Z"},"trusted":true},"execution_count":null,"outputs":[]}]}